import pymongo
from datetime import datetime
import logging

from bson import ObjectId

def serialize_mongo_document(document):
    """Convertit un document MongoDB en dictionnaire s√©rialisable par FastAPI."""
    if isinstance(document, list):
        return [serialize_mongo_document(doc) for doc in document]
    
    if isinstance(document, dict):
        document = document.copy()  # √âviter de modifier l'original
        if "_id" in document and isinstance(document["_id"], ObjectId):
            document["_id"] = str(document["_id"])  # Convertir ObjectId en string
        return document

    return document


# Configuration du logging
logger = logging.getLogger("MongoDB")

# Configuration de la connexion MongoDB
MONGO_URI = "mongodb://localhost:27017/"
DB_NAME = "twitch_data"
CATEGORIES_COLLECTION = "categories"
STREAMS_COLLECTION = "streams"

# Initialisation du client MongoDB
try:
    client = pymongo.MongoClient(MONGO_URI)
    db = client[DB_NAME]
    categories_collection = db[CATEGORIES_COLLECTION]
    streams_collection = db[STREAMS_COLLECTION]
    
    # Cr√©ation d'index pour les performances
    categories_collection.create_index([("category", pymongo.ASCENDING), ("timestamp", pymongo.DESCENDING)])
    streams_collection.create_index([("category", pymongo.ASCENDING), ("timestamp", pymongo.DESCENDING)])
    streams_collection.create_index([("channel", pymongo.ASCENDING), ("timestamp", pymongo.DESCENDING)])
    
    logger.info("Connexion √† MongoDB √©tablie avec succ√®s")
except Exception as e:
    logger.error(f"Erreur de connexion √† MongoDB: {e}")
    raise

def save_categories_to_db(categories_data):
    """Enregistre les donn√©es des cat√©gories dans MongoDB."""
    try:
        if not categories_data:
            logger.warning("Aucune donn√©e de cat√©gories √† sauvegarder")
            return
        
        # Ajouter une date de cr√©ation pour faciliter les requ√™tes
        for data in categories_data:
            data["created_at"] = datetime.now()
        
        # Ins√©rer les donn√©es
        result = categories_collection.insert_many(categories_data)
        logger.info(f"{len(result.inserted_ids)} cat√©gories enregistr√©es dans MongoDB")
        return result.inserted_ids
    except Exception as e:
        logger.error(f"Erreur lors de l'enregistrement des cat√©gories: {e}")
        return None

def save_streams_to_db(streams_data):
    """Enregistre les donn√©es des streamers dans MongoDB."""
    try:
        if not streams_data:
            logger.warning("Aucune donn√©e de streamers √† sauvegarder")
            return
        
        # Ajouter une date de cr√©ation pour faciliter les requ√™tes
        for data in streams_data:
            data["created_at"] = datetime.now()
        
        # Ins√©rer les donn√©es
        result = streams_collection.insert_many(streams_data)
        logger.info(f"{len(result.inserted_ids)} streamers enregistr√©s dans MongoDB")
        return result.inserted_ids
    except Exception as e:
        logger.error(f"Erreur lors de l'enregistrement des streamers: {e}")
        return None

def get_latest_categories(limit=100):
    """R√©cup√®re les derni√®res cat√©gories de la base de donn√©es."""
    try:
        pipeline = [
            {"$sort": {"created_at": -1}},
            {"$group": {
                "_id": "$category",
                "doc": {"$first": "$$ROOT"}
            }},
            {"$replaceRoot": {"newRoot": "$doc"}},
            {"$sort": {"viewers": -1}},
            {"$limit": limit}
        ]
        
        results = list(categories_collection.aggregate(pipeline))
        return serialize_mongo_document(results)  # üî• Appliquer la conversion
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des cat√©gories: {e}")
        return []

def get_latest_streams(category=None, limit=100):
    """R√©cup√®re les derniers streams de la base de donn√©es."""
    try:
        query = {"category": category} if category else {}

        if category:
            results = list(streams_collection.find(query).sort("viewers", -1).limit(limit))
        else:
            # Si aucune cat√©gorie n'est sp√©cifi√©e, r√©cup√©rer les derniers streams toutes cat√©gories confondues
            pipeline = [
                {"$sort": {"created_at": -1}},
                {"$group": {
                    "_id": "$channel",
                    "doc": {"$first": "$$ROOT"}
                }},
                {"$replaceRoot": {"newRoot": "$doc"}},
                {"$sort": {"viewers": -1}},
                {"$limit": limit}
            ]
            results = list(streams_collection.aggregate(pipeline))

        return serialize_mongo_document(results)  # üî• Convertir ObjectId en str
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des streams: {e}")
        return []

def get_categories_history(hours=24):
    """R√©cup√®re l'historique des cat√©gories sur une p√©riode donn√©e."""
    try:
        # Calculer la date limite
        limit_date = datetime.now()
        limit_date = limit_date.replace(hour=limit_date.hour - hours)
        
        # Requ√™te pour obtenir l'historique
        pipeline = [
            {"$match": {"created_at": {"$gt": limit_date}}},
            {"$group": {
                "_id": {
                    "category": "$category",
                    "hour": {"$hour": "$created_at"},
                    "day": {"$dayOfMonth": "$created_at"},
                    "month": {"$month": "$created_at"}
                },
                "avg_viewers": {"$avg": "$viewers"},
                "max_viewers": {"$max": "$viewers"},
                "min_viewers": {"$min": "$viewers"},
                "count": {"$sum": 1}
            }},
            {"$sort": {"_id.month": 1, "_id.day": 1, "_id.hour": 1}}
        ]
        
        results = list(categories_collection.aggregate(pipeline))
        logger.info(f"R√©cup√©r√© {len(results)} points de donn√©es historiques pour les cat√©gories")
        return results
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration de l'historique des cat√©gories: {e}")
        return []

def get_streams_history(category=None, hours=24):
    """R√©cup√®re l'historique des streams sur une p√©riode donn√©e."""
    try:
        # Calculer la date limite
        limit_date = datetime.now()
        limit_date = limit_date.replace(hour=limit_date.hour - hours)
        
        # Construire la requ√™te
        match_query = {"created_at": {"$gt": limit_date}}
        if category:
            match_query["category"] = category
        
        # Pipeline d'agr√©gation
        pipeline = [
            {"$match": match_query},
            {"$group": {
                "_id": {
                    "channel": "$channel",
                    "hour": {"$hour": "$created_at"},
                    "day": {"$dayOfMonth": "$created_at"},
                    "month": {"$month": "$created_at"}
                },
                "avg_viewers": {"$avg": "$viewers"},
                "max_viewers": {"$max": "$viewers"},
                "min_viewers": {"$min": "$viewers"},
                "count": {"$sum": 1}
            }},
            {"$sort": {"_id.month": 1, "_id.day": 1, "_id.hour": 1}}
        ]
        
        results = list(streams_collection.aggregate(pipeline))
        logger.info(f"R√©cup√©r√© {len(results)} points de donn√©es historiques pour les streams" + 
                   (f" de {category}" if category else ""))
        return results
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration de l'historique des streams: {e}")
        return []
